{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(3, 8)\n",
            "132\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "y = [[2,3,4,5,6,7,8,9], [2,3,4,5,6,7,8,9], [2,3,4,5,6,7,8,9]]\n",
        "x = np.array(y)\n",
        "print(x.shape)\n",
        "print(np.sum(x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "_aeq-naZkPPU",
        "outputId": "cdfe2fdd-cc93-4370-9345-e06ddda1e4ce"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "# # !ls 'content/drive/MyDrive/'\n",
        "# import csv\n",
        "\n",
        "with open('/content/drive/MyDrive/train.csv', mode = 'r') as file:\n",
        "  # trainfile = csv.reader(file)\n",
        "  trainfile = pd.read_csv(file)\n",
        "\n",
        "\n",
        "# Missing value columns are identified.\n",
        "missing_val_col_list = []\n",
        "col = trainfile.shape[1]-1\n",
        "\n",
        "\n",
        "# /// 1. Find and modify the missing values in the data file. \\\\\\\n",
        "\n",
        "for i in range(1,col):\n",
        "  if(trainfile.iloc[:, i].isnull().sum()):\n",
        "    missing_val_col_list.append(i)\n",
        "# print(missing_val_col_list)\n",
        "\n",
        "\n",
        "# Segregating classification type and numerical type missing value column.\n",
        "missing_val_ctype_col_list = []\n",
        "missing_val_ntype_col_list = []\n",
        "for j in missing_val_col_list:\n",
        "  if trainfile.iloc[:, j].dtype == 'float':\n",
        "    missing_val_ntype_col_list.append(j)\n",
        "  else:\n",
        "    missing_val_ctype_col_list.append(j)\n",
        "print(\"///////1. trainfile\\\\\\\\\\\\\")\n",
        "print(\"Classification type column->\", missing_val_ctype_col_list)\n",
        "print(\"Numerical type column->\", missing_val_ntype_col_list)\n",
        "\n",
        "\n",
        "# Listing the missing data percentage of the column and removing  if more than 45% is missing.\n",
        "missing_percent = trainfile.iloc[:,1:col].isnull().mean()*100\n",
        "for i in missing_percent:\n",
        "  if i > 0:\n",
        "    print(i)\n",
        "trainfile_cleaned = trainfile.drop(columns = missing_percent[missing_percent>45].index)\n",
        "\n",
        "missing_val_col = []\n",
        "mod_col = trainfile_cleaned.shape[1]\n",
        "for i in range(1,mod_col):\n",
        "  if(trainfile_cleaned.iloc[:, i].isnull().sum()):\n",
        "    missing_val_col.append(i)\n",
        "# print(missing_val_col)\n",
        "\n",
        "missing_val_ctype_col = []\n",
        "missing_val_ntype_col = []\n",
        "for j in missing_val_col:\n",
        "  if trainfile_cleaned.iloc[:, j].dtype == 'float':\n",
        "    missing_val_ntype_col.append(j)\n",
        "  else:\n",
        "    missing_val_ctype_col.append(j)\n",
        "print(\"///////2. trainfile_cleaned\\\\\\\\\\\\\")\n",
        "print(\"Modified Classification type column->\", missing_val_ctype_col)\n",
        "print(\"Modified Numerical type column->\", missing_val_ntype_col)\n",
        "\n",
        "missing_percentage = trainfile_cleaned.iloc[:,1:mod_col].isnull().mean()*100\n",
        "for i in missing_percentage:\n",
        "  if i > 0:\n",
        "    print(i)\n",
        "\n",
        "\n",
        "# /// 2. Filling up missing columns with median(for numerical data) and mode(for classification data).\\\\\\\n",
        "\n",
        "print(\"///////3. trainfile_cleaned\\\\\\\\\\\\\")\n",
        "print(\"\")\n",
        "print(\"Classification data sets.\")\n",
        "print(\"\")\n",
        "for k in missing_val_ctype_col:\n",
        "  trainfile_cleaned.iloc[:, k] = trainfile_cleaned.iloc[:, k].fillna(trainfile_cleaned.iloc[:, k].mode()[0])\n",
        "  print(trainfile_cleaned.iloc[:, k].describe(include = ['object']))\n",
        "\n",
        "print(\"\")\n",
        "print(\"Numerical data sets.\")\n",
        "print(\"\")\n",
        "for k in missing_val_ntype_col:\n",
        "  trainfile_cleaned.iloc[:, k] = trainfile_cleaned.iloc[:, k].fillna(trainfile_cleaned.iloc[:, k].median())\n",
        "  print(trainfile_cleaned.iloc[:, k].describe())\n",
        "\n",
        "for i in range(len(missing_val_ntype_col)):\n",
        "  # sns.histplot(trainfile_cleaned.iloc[:, missing_val_ntype_col[i]], kde = True, bins = 120)\n",
        "  # sns.boxplot(x = trainfile_cleaned.iloc[:, missing_val_ntype_col[i]])\n",
        "  y_value = trainfile_cleaned.iloc[:, missing_val_ntype_col[i]]\n",
        "  x_value = range(len(y_value))\n",
        "  plt.scatter(x_value, y_value, color = 'blue', alpha = 0.5)\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "# \\\\\\DEBUG CODE/// ->To check that all missing data's are filled.\n",
        "# print(\"//////////////////////////////////////////////////////////////////////////////\")\n",
        "print(\"///////4. trainfile_cleaned\\\\\\\\\\\\\")\n",
        "print(\"\")\n",
        "print(\"Classification data sets.\")\n",
        "print(\"\")\n",
        "for k in missing_val_ctype_col:\n",
        "  print(trainfile_cleaned.iloc[:, k].describe(include = ['object']))\n",
        "print(\"\")\n",
        "print(\"Numerical data sets.\")\n",
        "print(\"\")\n",
        "for k in missing_val_ntype_col:\n",
        "  print(trainfile_cleaned.iloc[:, k].describe())\n",
        "\n",
        "\n",
        "classification_col = trainfile_cleaned.select_dtypes(include=['object', 'category']).columns\n",
        "classification_indices = [trainfile_cleaned.columns.get_loc(col) for col in classification_col]\n",
        "\n",
        "numerical_col = trainfile_cleaned.select_dtypes(include=['number']).columns\n",
        "numerical_indices = [trainfile_cleaned.columns.get_loc(col) for col in numerical_col]\n",
        "\n",
        "print(f\"{classification_indices} total: {len(classification_indices)}\")\n",
        "\n",
        "for i in range(len(classification_col)):\n",
        "  x = trainfile_cleaned[numerical_col[len(numerical_col) - 1]]\n",
        "  y = trainfile_cleaned[classification_col[i]]\n",
        "  plt.scatter(y, x, color = 'blue', alpha = 0.5)\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Odk2MCrwJo-3",
        "outputId": "fd2d5f04-6130-49a6-9811-c8c2e3d8731e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Training data shape: (1168, 267)\n",
            "Testing data shape: (292, 267)\n",
            "Mean Squared Error: 852273023.2856407\n",
            "R-squared: 0.8888869912094407\n",
            "Fitting 5 folds for each of 81 candidates, totalling 405 fits\n",
            "Best Hyperparameters: {'max_depth': 30, 'min_samples_leaf': 1, 'min_samples_split': 5, 'n_estimators': 100}\n",
            "Mean Squared Error on Test Set: 853891109.0901171\n",
            "R-squared on Test Set: 0.888676037234242\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Read the data\n",
        "with open('/content/drive/MyDrive/train.csv', mode='r') as file:\n",
        "    trainfile = pd.read_csv(file)\n",
        "\n",
        "# Identifying columns with missing values\n",
        "missing_val_col_list = []\n",
        "col = trainfile.shape[1] - 1\n",
        "\n",
        "for i in range(1, col):\n",
        "    if trainfile.iloc[:, i].isnull().sum():\n",
        "        missing_val_col_list.append(i)\n",
        "\n",
        "# Separating numerical and categorical columns\n",
        "missing_val_ctype_col_list = []\n",
        "missing_val_ntype_col_list = []\n",
        "for j in missing_val_col_list:\n",
        "    if pd.api.types.is_numeric_dtype(trainfile.iloc[:, j]):\n",
        "        missing_val_ntype_col_list.append(j)\n",
        "    else:\n",
        "        missing_val_ctype_col_list.append(j)\n",
        "\n",
        "# Calculate the percentage of missing values and drop columns with >45% missing values\n",
        "missing_percent = trainfile.iloc[:, 1:col].isnull().mean() * 100\n",
        "trainfile_cleaned = trainfile.drop(columns=missing_percent[missing_percent > 45].index)\n",
        "\n",
        "# Identify the columns with remaining missing values in the cleaned dataset\n",
        "missing_val_col = []\n",
        "mod_col = trainfile_cleaned.shape[1]\n",
        "\n",
        "for i in range(1, mod_col):\n",
        "    if trainfile_cleaned.iloc[:, i].isnull().sum():\n",
        "        missing_val_col.append(i)\n",
        "\n",
        "# Separating numerical and categorical columns again\n",
        "missing_val_ctype_col = []\n",
        "missing_val_ntype_col = []\n",
        "for j in missing_val_col:\n",
        "    if pd.api.types.is_numeric_dtype(trainfile_cleaned.iloc[:, j]):\n",
        "        missing_val_ntype_col.append(j)\n",
        "    else:\n",
        "        missing_val_ctype_col.append(j)\n",
        "\n",
        "# Fill missing values for categorical and numerical data\n",
        "for k in missing_val_ctype_col:\n",
        "    trainfile_cleaned.iloc[:, k] = trainfile_cleaned.iloc[:, k].fillna(trainfile_cleaned.iloc[:, k].mode()[0])\n",
        "\n",
        "for k in missing_val_ntype_col:\n",
        "    trainfile_cleaned.iloc[:, k] = trainfile_cleaned.iloc[:, k].fillna(trainfile_cleaned.iloc[:, k].median())\n",
        "\n",
        "# One-Hot Encoding for categorical columns\n",
        "classification_col = trainfile_cleaned.select_dtypes(include=['object', 'category']).columns\n",
        "\n",
        "# Apply One-Hot Encoding\n",
        "trainfile_encoded = pd.get_dummies(trainfile_cleaned, columns=classification_col)\n",
        "\n",
        "# Display the updated dataframe with encoded columns\n",
        "# print(\"After One-Hot Encoding:\")\n",
        "# print(trainfile_encoded.head())\n",
        "\n",
        "# To check how many columns are created after encoding\n",
        "# print(f\"Total number of columns after encoding: {trainfile_encoded.shape[1]}\")\n",
        "\n",
        "# Assume 'target_column' is the column you want to predict (i.e., your dependent variable)\n",
        "X = trainfile_encoded.drop('SalePrice', axis=1)  # Features (all columns except the target)\n",
        "y = trainfile_encoded['SalePrice']  # Target (dependent variable)\n",
        "\n",
        "# Split the data into training and testing sets (e.g., 80% training, 20% testing)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "print(f\"Training data shape: {X_train.shape}\")\n",
        "print(f\"Testing data shape: {X_test.shape}\")\n",
        "\n",
        "# Random Forest Regressor\n",
        "model = RandomForestRegressor(random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predicting using test data.\n",
        "y_pred = model.predict(X_test)\n",
        "# print(y_pred)\n",
        "\n",
        "# Calculating mse and rsquare for regression data.\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "print(f\"Mean Squared Error: {mse}\")\n",
        "print(f\"R-squared: {r2}\")\n",
        "\n",
        "# Hyper parameter tuning/ cross validation.\n",
        "# Define the hyperparameter grid to search\n",
        "param_grid = {\n",
        "    'n_estimators': [100, 200, 300],  # Number of trees\n",
        "    'max_depth': [10, 20, 30],        # Depth of trees\n",
        "    'min_samples_split': [2, 5, 10],  # Minimum samples required to split a node\n",
        "    'min_samples_leaf': [1, 2, 4]     # Minimum samples required at a leaf node\n",
        "}\n",
        "\n",
        "# Initialize the model\n",
        "model = RandomForestRegressor(random_state=42)\n",
        "\n",
        "# GridSearchCV to find the best combination of hyperparameters\n",
        "grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=5, scoring='neg_mean_squared_error', verbose=1, n_jobs=-1)\n",
        "\n",
        "# Fit the grid search model\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Get the best hyperparameters\n",
        "best_params = grid_search.best_params_\n",
        "print(f\"Best Hyperparameters: {best_params}\")\n",
        "\n",
        "# Get the best model from Grid Search or Randomized Search\n",
        "best_model = grid_search.best_estimator_\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = best_model.predict(X_test)\n",
        "\n",
        "# Evaluate the performance of the model\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "print(f\"Mean Squared Error on Test Set: {mse}\")\n",
        "print(f\"R-squared on Test Set: {r2}\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V28",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
