{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9303341d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python311\\Lib\\site-packages\\pygame\\pkgdata.py:25: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  from pkg_resources import resource_stream, resource_exists\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.6.1 (SDL 2.28.4, Python 3.13.9)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "import pygame, random, math, time, os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from collections import deque, namedtuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e9ad9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "SCREEN_W, SCREEN_H = 600, 400\n",
    "ROBOT_R = 10\n",
    "GOAL_R = 12\n",
    "NUM_OBS = 8\n",
    "MAX_EPISODE_STEPS = 200\n",
    "GAMMA = 0.99\n",
    "BATCH_SIZE = 64\n",
    "LR = 1e-3\n",
    "BUFFER_SIZE = 20000\n",
    "MIN_REPLAY = 500\n",
    "EPS_START, EPS_END, EPS_DECAY = 1.0, 0.05, 50000\n",
    "TRAIN_EPISODES_PER_LOOP = 4\n",
    "MODEL_PATH = \"dqn_nav.pth\"\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class NavEnv:\n",
    "    def __init__(self):\n",
    "        self.w, self.h = SCREEN_W, SCREEN_H\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.robot = np.array([random.choice([40, self.w-40]), random.choice([40, self.h-40])], dtype=np.float32)\n",
    "        \n",
    "        while True:\n",
    "            self.goal = np.array([random.uniform(60, self.w-60), random.uniform(60, self.h-60)], dtype=np.float32)\n",
    "            if np.linalg.norm(self.goal - self.robot) > 120:\n",
    "                break\n",
    "        \n",
    "        self.obs = []\n",
    "        for _ in range(NUM_OBS):\n",
    "            ow, oh = random.randint(30,80), random.randint(20,60)\n",
    "            ox = random.uniform(60, self.w-60-ow)\n",
    "            oy = random.uniform(60, self.h-60-oh)\n",
    "            self.obs.append((ox,oy,ow,oh))\n",
    "        self.t = 0\n",
    "        return self._get_obs()\n",
    "\n",
    "    def _get_obs(self):\n",
    "        obs = [self.robot[0]/self.w, self.robot[1]/self.h, self.goal[0]/self.w, self.goal[1]/self.h]\n",
    "        for (ox,oy,ow,oh) in self.obs:\n",
    "            cx = (ox+ow/2)/self.w\n",
    "            cy = (oy+oh/2)/self.h\n",
    "            obs += [cx, cy, ow/self.w, oh/self.h]\n",
    "        return np.array(obs, dtype=np.float32)\n",
    "\n",
    "    def step(self, action):\n",
    "        step_size = 4.5\n",
    "        if action==0: self.robot[1] -= step_size\n",
    "        elif action==1: self.robot[1] += step_size\n",
    "        elif action==2: self.robot[0] -= step_size\n",
    "        elif action==3: self.robot[0] += step_size\n",
    "        self.robot = np.clip(self.robot, [ROBOT_R, ROBOT_R], [self.w-ROBOT_R, self.h-ROBOT_R])\n",
    "        self.t += 1\n",
    "        done = False\n",
    "        reward = -0.01\n",
    "        for (ox,oy,ow,oh) in self.obs:\n",
    "            if ox <= self.robot[0] <= ox+ow and oy <= self.robot[1] <= oy+oh:\n",
    "                reward -= 1.0\n",
    "                done = True\n",
    "        if np.linalg.norm(self.robot - self.goal) < (ROBOT_R + GOAL_R):\n",
    "            reward += 2.0\n",
    "            done = True\n",
    "        if self.t >= MAX_EPISODE_STEPS:\n",
    "            done = True\n",
    "        return self._get_obs(), reward, done, {}\n",
    "\n",
    "Transition = namedtuple('Transition', ('s','a','r','s2','d'))\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity=BUFFER_SIZE):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "    def push(self, *args): self.buffer.append(Transition(*args))\n",
    "    def sample(self, n):\n",
    "        batch = random.sample(self.buffer, n)\n",
    "        return Transition(*zip(*batch))\n",
    "    def __len__(self): return len(self.buffer)\n",
    "\n",
    "class QNet(nn.Module):\n",
    "    def __init__(self, input_dim, n_actions):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim,128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128,128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128,n_actions)\n",
    "        )\n",
    "    def forward(self,x): return self.net(x)\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self, obs_dim, n_actions):\n",
    "        self.n_actions = n_actions\n",
    "        self.policy = QNet(obs_dim, n_actions).to(DEVICE)\n",
    "        self.target = QNet(obs_dim, n_actions).to(DEVICE)\n",
    "        self.target.load_state_dict(self.policy.state_dict())\n",
    "        self.opt = optim.Adam(self.policy.parameters(), lr=LR)\n",
    "        self.buffer = ReplayBuffer()\n",
    "        self.steps = 0\n",
    "        self.eps = EPS_START\n",
    "\n",
    "    def act(self, obs, eval=False):\n",
    "        self.steps += 1\n",
    "        if not eval and random.random() < self.eps:\n",
    "            return random.randrange(self.n_actions)\n",
    "        with torch.no_grad():\n",
    "            x = torch.tensor(obs, dtype=torch.float32, device=DEVICE).unsqueeze(0)\n",
    "            q = self.policy(x)\n",
    "            return int(q.argmax().item())\n",
    "\n",
    "    def push(self, *transition): self.buffer.push(*transition)\n",
    "\n",
    "    def update(self):\n",
    "        if len(self.buffer) < MIN_REPLAY:\n",
    "            return 0.0\n",
    "        batch = self.buffer.sample(BATCH_SIZE)\n",
    "        s = torch.tensor(np.array(batch.s), dtype=torch.float32, device=DEVICE)\n",
    "        a = torch.tensor(batch.a, dtype=torch.int64, device=DEVICE).unsqueeze(1)\n",
    "        r = torch.tensor(batch.r, dtype=torch.float32, device=DEVICE).unsqueeze(1)\n",
    "        s2 = torch.tensor(np.array(batch.s2), dtype=torch.float32, device=DEVICE)\n",
    "        d = torch.tensor(batch.d, dtype=torch.float32, device=DEVICE).unsqueeze(1)\n",
    "        q = self.policy(s).gather(1,a)\n",
    "        with torch.no_grad():\n",
    "            q2 = self.target(s2).max(1)[0].unsqueeze(1)\n",
    "            target = r + (1-d)*GAMMA*q2\n",
    "        loss = nn.functional.mse_loss(q, target)\n",
    "        self.opt.zero_grad(); loss.backward(); self.opt.step()\n",
    "        # epsilon decay\n",
    "        self.eps = max(EPS_END, EPS_START - self.steps/ EPS_DECAY)\n",
    "        # soft update\n",
    "        for p, tp in zip(self.policy.parameters(), self.target.parameters()):\n",
    "            tp.data.copy_(0.995*tp.data + 0.005*p.data)\n",
    "        return loss.item()\n",
    "\n",
    "    def save(self, path=MODEL_PATH):\n",
    "        torch.save(self.policy.state_dict(), path)\n",
    "\n",
    "    def load(self, path=MODEL_PATH):\n",
    "        if os.path.exists(path):\n",
    "            self.policy.load_state_dict(torch.load(path, map_location=DEVICE))\n",
    "            self.target.load_state_dict(self.policy.state_dict())\n",
    "            print(\"Model loaded:\", path)\n",
    "        else:\n",
    "            print(\"No model found at\", path)\n",
    "\n",
    "def draw_env(screen, env):\n",
    "    screen.fill((30,30,30))\n",
    "    # obstacles\n",
    "    for (ox,oy,ow,oh) in env.obs:\n",
    "        pygame.draw.rect(screen,(120,60,60), pygame.Rect(int(ox),int(oy),int(ow),int(oh)))\n",
    "    # goal\n",
    "    pygame.draw.circle(screen, (50,200,50), (int(env.goal[0]), int(env.goal[1])), GOAL_R)\n",
    "    # robot\n",
    "    pygame.draw.circle(screen, (200,200,50), (int(env.robot[0]), int(env.robot[1])), ROBOT_R)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "14685af9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mode: train\n",
      "Episode 20, eps 0.050, buffer 3435\n",
      "Episode 40, eps 0.050, buffer 7175\n",
      "Episode 60, eps 0.050, buffer 10979\n",
      "Episode 80, eps 0.050, buffer 14793\n",
      "Episode 100, eps 0.050, buffer 18709\n",
      "Episode 120, eps 0.050, buffer 20000\n",
      "Mode: play\n",
      "Mode: train\n",
      "Episode 140, eps 0.050, buffer 20000\n",
      "Episode 160, eps 0.050, buffer 20000\n",
      "Episode 180, eps 0.050, buffer 20000\n",
      "Mode: play\n",
      "Mode: train\n",
      "Episode 200, eps 0.050, buffer 20000\n",
      "Episode 220, eps 0.050, buffer 20000\n",
      "Episode 240, eps 0.050, buffer 20000\n",
      "Episode 260, eps 0.050, buffer 20000\n",
      "Episode 280, eps 0.050, buffer 20000\n",
      "Episode 300, eps 0.050, buffer 20000\n",
      "Episode 320, eps 0.050, buffer 20000\n",
      "Mode: idle\n",
      "Mode: play\n",
      "Mode: play\n",
      "Mode: play\n",
      "Mode: play\n",
      "Mode: play\n",
      "Mode: train\n",
      "Episode 340, eps 0.050, buffer 20000\n",
      "Episode 360, eps 0.050, buffer 20000\n",
      "Episode 380, eps 0.050, buffer 20000\n",
      "Episode 400, eps 0.050, buffer 20000\n",
      "Episode 420, eps 0.050, buffer 20000\n",
      "Episode 440, eps 0.050, buffer 20000\n",
      "Episode 460, eps 0.050, buffer 20000\n",
      "Episode 480, eps 0.050, buffer 20000\n",
      "Episode 500, eps 0.050, buffer 20000\n",
      "Mode: idle\n",
      "Mode: play\n",
      "Mode: play\n",
      "Mode: play\n",
      "Mode: play\n",
      "Mode: play\n",
      "Mode: play\n",
      "No model found at dqn_nav.pth\n",
      "Mode: play\n",
      "No model found at dqn_nav.pth\n",
      "Mode: play\n",
      "Mode: train\n",
      "Episode 520, eps 0.050, buffer 20000\n",
      "Episode 540, eps 0.050, buffer 20000\n",
      "Episode 560, eps 0.050, buffer 20000\n",
      "Episode 580, eps 0.050, buffer 20000\n",
      "Episode 600, eps 0.050, buffer 20000\n",
      "Episode 620, eps 0.050, buffer 20000\n",
      "Episode 640, eps 0.050, buffer 20000\n",
      "Episode 660, eps 0.050, buffer 20000\n",
      "Episode 680, eps 0.050, buffer 20000\n",
      "Episode 700, eps 0.050, buffer 20000\n",
      "Episode 720, eps 0.050, buffer 20000\n",
      "Episode 740, eps 0.050, buffer 20000\n",
      "Episode 760, eps 0.050, buffer 20000\n",
      "Episode 780, eps 0.050, buffer 20000\n",
      "Episode 800, eps 0.050, buffer 20000\n",
      "Episode 820, eps 0.050, buffer 20000\n",
      "Episode 840, eps 0.050, buffer 20000\n",
      "Episode 860, eps 0.050, buffer 20000\n",
      "Episode 880, eps 0.050, buffer 20000\n",
      "Episode 900, eps 0.050, buffer 20000\n",
      "Episode 920, eps 0.050, buffer 20000\n",
      "Episode 940, eps 0.050, buffer 20000\n",
      "Episode 960, eps 0.050, buffer 20000\n",
      "Episode 980, eps 0.050, buffer 20000\n",
      "Episode 1000, eps 0.050, buffer 20000\n",
      "Episode 1020, eps 0.050, buffer 20000\n",
      "Episode 1040, eps 0.050, buffer 20000\n",
      "Mode: play\n",
      "Mode: train\n",
      "Episode 1060, eps 0.050, buffer 20000\n",
      "Mode: idle\n",
      "Mode: play\n",
      "Mode: play\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    pygame.init()\n",
    "    screen = pygame.display.set_mode((SCREEN_W, SCREEN_H))\n",
    "    pygame.display.set_caption(\"RL Nav (DQN) - press t to train, p play, s save, l load, r reset\")\n",
    "    clock = pygame.time.Clock()\n",
    "    env = NavEnv()\n",
    "    obs0 = env.reset()\n",
    "    agent = Agent(obs0.shape[0], 4)\n",
    "    mode = \"idle\"  # 'train', 'play'\n",
    "    running = True\n",
    "    train_losses = []\n",
    "    episode = 0\n",
    "\n",
    "    while running:\n",
    "        for event in pygame.event.get():\n",
    "            if event.type==pygame.QUIT:\n",
    "                running=False\n",
    "            elif event.type==pygame.KEYDOWN:\n",
    "                if event.key==pygame.K_t:\n",
    "                    mode = \"train\" if mode!=\"train\" else \"idle\"\n",
    "                    print(\"Mode:\", mode)\n",
    "                elif event.key==pygame.K_p:\n",
    "                    mode = \"play\" if mode!=\"play\" else \"idle\"\n",
    "                    print(\"Mode:\", mode)\n",
    "                elif event.key==pygame.K_r:\n",
    "                    obs0 = env.reset()\n",
    "                    episode = 0\n",
    "                    print(\"Reset\")\n",
    "                elif event.key==pygame.K_s:\n",
    "                    agent.save()\n",
    "                    print(\"Saved model.\")\n",
    "                elif event.key==pygame.K_l:\n",
    "                    agent.load()\n",
    "        if mode==\"train\":\n",
    "            for _ in range(TRAIN_EPISODES_PER_LOOP):\n",
    "                s = env.reset()\n",
    "                done=False\n",
    "                ep_reward=0.0\n",
    "                while not done:\n",
    "                    a = agent.act(s, eval=False)\n",
    "                    s2, r, done, _ = env.step(a)\n",
    "                    agent.push(s,a,r,s2, float(done))\n",
    "                    loss = agent.update()\n",
    "                    s = s2\n",
    "                    ep_reward += r\n",
    "                episode +=1\n",
    "            if episode % 10 == 0:\n",
    "                print(f\"Episode {episode}, eps {agent.eps:.3f}, buffer {len(agent.buffer)}\")\n",
    "        elif mode==\"play\":\n",
    "            s = env.reset()\n",
    "            done=False\n",
    "            while not done and mode==\"play\":\n",
    "                for ev in pygame.event.get():\n",
    "                    if ev.type==pygame.QUIT:\n",
    "                        running=False\n",
    "                        mode=\"idle\"\n",
    "                a = agent.act(s, eval=True)\n",
    "                s2, r, done, _ = env.step(a)\n",
    "                s = s2\n",
    "                draw_env(screen, env)\n",
    "                pygame.display.flip()\n",
    "                clock.tick(60)\n",
    "            mode=\"idle\"\n",
    "        draw_env(screen, env)\n",
    "        font = pygame.font.SysFont(\"Arial\", 14)\n",
    "        txt = font.render(f\"Mode: {mode}  Episodes: {episode}  Eps: {agent.eps:.2f}\", True, (230,230,230))\n",
    "        screen.blit(txt, (8,8))\n",
    "        pygame.display.flip()\n",
    "        clock.tick(60)\n",
    "    pygame.quit()\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cfc06f6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
